{"total_count": 16, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/bigcode-project/starcoder.cpp/issues/36", "repository_url": "https://api.github.com/repos/bigcode-project/starcoder.cpp", "labels_url": "https://api.github.com/repos/bigcode-project/starcoder.cpp/issues/36/labels{/name}", "comments_url": "https://api.github.com/repos/bigcode-project/starcoder.cpp/issues/36/comments", "events_url": "https://api.github.com/repos/bigcode-project/starcoder.cpp/issues/36/events", "html_url": "https://github.com/bigcode-project/starcoder.cpp/issues/36", "id": 2770959382, "node_id": "I_kwDOJjJ95c6lKXwW", "number": 36, "title": "convert-hf-to-ggml.py offload_weight() too much parameters", "user": {"login": "dmeziere", "id": 3167934, "node_id": "MDQ6VXNlcjMxNjc5MzQ=", "avatar_url": "https://avatars.githubusercontent.com/u/3167934?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dmeziere", "html_url": "https://github.com/dmeziere", "followers_url": "https://api.github.com/users/dmeziere/followers", "following_url": "https://api.github.com/users/dmeziere/following{/other_user}", "gists_url": "https://api.github.com/users/dmeziere/gists{/gist_id}", "starred_url": "https://api.github.com/users/dmeziere/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dmeziere/subscriptions", "organizations_url": "https://api.github.com/users/dmeziere/orgs", "repos_url": "https://api.github.com/users/dmeziere/repos", "events_url": "https://api.github.com/users/dmeziere/events{/privacy}", "received_events_url": "https://api.github.com/users/dmeziere/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2025-01-06T16:01:27Z", "updated_at": "2025-01-06T16:01:27Z", "closed_at": null, "author_association": "NONE", "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "active_lock_reason": null, "body": "I tried to follow the _Quick start_.\r\n\r\n```\r\n$ git clone https://github.com/bigcode-project/starcoder.cpp\r\nClonage dans 'starcoder.cpp'...\r\nremote: Enumerating objects: 110, done.\r\nremote: Counting objects: 100% (110/110), done.\r\nremote: Compressing objects: 100% (78/78), done.\r\nremote: Total 110 (delta 34), reused 93 (delta 25), pack-reused 0 (from 0)\r\nR\u00e9ception d'objets: 100% (110/110), 7.28 Mio | 28.89 Mio/s, fait.\r\nR\u00e9solution des deltas: 100% (34/34), fait.\r\n```\r\n```\r\n$ cd starcoder.cpp/\r\n```\r\n```\r\n$ python convert-hf-to-ggml.py bigcode/gpt_bigcode-santacoder\r\nLoading model:  bigcode/gpt_bigcode-santacoder\r\ntokenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 159/159 [00:00<00:00, 324kB/s]\r\ntokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.08M/2.08M [00:00<00:00, 6.11MB/s]\r\nspecial_tokens_map.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 138/138 [00:00<00:00, 387kB/s]\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nconfig.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 812/812 [00:00<00:00, 2.52MB/s]\r\nmodel.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.25G/2.25G [00:43<00:00, 52.3MB/s]\r\nTraceback (most recent call last):\r\n  File \"/mnt/hdd/Data/ia/starcoder.cpp/convert-hf-to-ggml.py\", line 58, in <module>\r\n    model = AutoModelForCausalLM.from_pretrained(model_name, config=config, torch_dtype=torch.float16 if use_f16 else torch.float32, low_cpu_mem_usage=True, trust_remote_code=True, offload_state_dict=True)\r\n  File \"/home/dmeziere/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 563, in from_pretrained\r\n    return model_class.from_pretrained(\r\n  File \"/home/dmeziere/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3507, in from_pretrained\r\n    ) = cls._load_pretrained_model(\r\n  File \"/home/dmeziere/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3932, in _load_pretrained_model\r\n    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\r\n  File \"/home/dmeziere/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 798, in _load_state_dict_into_meta_model\r\n    state_dict_index = offload_weight(param, param_name, model, state_dict_folder, state_dict_index)\r\nTypeError: offload_weight() takes from 3 to 4 positional arguments but 5 were given\r\n```\r\nThen i tried `make`, but it also returns a lot of warnings. But i think it has compiled.\r\n\r\n```\r\n$ ./quantize models/bigcode/gpt_bigcode-santacoder-ggml.bin models/bigcode/gpt_bigcode-santacoder-ggml-q4_1.bin 3\r\nstarcoder_model_quantize: loading model from 'models/bigcode/gpt_bigcode-santacoder-ggml.bin'\r\nstarcoder_model_quantize: failed to open 'models/bigcode/gpt_bigcode-santacoder-ggml.bin' for reading\r\nmain: failed to quantize model from 'models/bigcode/gpt_bigcode-santacoder-ggml.bin'\r\n```\r\n\r\nThe model seems to be missing, probably because of the errors of `convert-hf-to-ggml.py`.", "reactions": {"url": "https://api.github.com/repos/bigcode-project/starcoder.cpp/issues/36/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/bigcode-project/starcoder.cpp/issues/36/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}]}