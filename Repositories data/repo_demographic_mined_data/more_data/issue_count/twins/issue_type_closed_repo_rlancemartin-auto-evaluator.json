{"total_count": 7, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/rlancemartin/auto-evaluator/issues/15", "repository_url": "https://api.github.com/repos/rlancemartin/auto-evaluator", "labels_url": "https://api.github.com/repos/rlancemartin/auto-evaluator/issues/15/labels{/name}", "comments_url": "https://api.github.com/repos/rlancemartin/auto-evaluator/issues/15/comments", "events_url": "https://api.github.com/repos/rlancemartin/auto-evaluator/issues/15/events", "html_url": "https://github.com/rlancemartin/auto-evaluator/issues/15", "id": 1684915019, "node_id": "I_kwDOJXAiPs5kbcNL", "number": 15, "title": "Is there a limit on the size of the PDF? A PDF with a size of 200kb reported an error", "user": {"login": "codehelen", "id": 58715436, "node_id": "MDQ6VXNlcjU4NzE1NDM2", "avatar_url": "https://avatars.githubusercontent.com/u/58715436?v=4", "gravatar_id": "", "url": "https://api.github.com/users/codehelen", "html_url": "https://github.com/codehelen", "followers_url": "https://api.github.com/users/codehelen/followers", "following_url": "https://api.github.com/users/codehelen/following{/other_user}", "gists_url": "https://api.github.com/users/codehelen/gists{/gist_id}", "starred_url": "https://api.github.com/users/codehelen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/codehelen/subscriptions", "organizations_url": "https://api.github.com/users/codehelen/orgs", "repos_url": "https://api.github.com/users/codehelen/repos", "events_url": "https://api.github.com/users/codehelen/events{/privacy}", "received_events_url": "https://api.github.com/users/codehelen/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2023-04-26T12:15:52Z", "updated_at": "2023-05-02T17:24:03Z", "closed_at": "2023-05-02T17:24:03Z", "author_association": "NONE", "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "active_lock_reason": null, "body": "Traceback (most recent call last):\r\n  File \"/Users/helen/code/xiaomi/auto-evaluator/venv/lib/python3.9/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 565, in _run_script\r\n    exec(code, module.__dict__)\r\n  File \"/Users/helen/code/xiaomi/auto-evaluator/auto-evaluator.py\", line 430, in <module>\r\n    graded_answers, graded_retrieval, latency, predictions = run_evaluation(qa_chain, retriever, eval_set, grade_prompt,\r\n  File \"/Users/helen/code/xiaomi/auto-evaluator/auto-evaluator.py\", line 342, in run_evaluation\r\n    retrieval_grade = grade_model_retrieval(gt_dataset, retrieved_docs, grade_prompt)\r\n  File \"/Users/helen/code/xiaomi/auto-evaluator/auto-evaluator.py\", line 277, in grade_model_retrieval\r\n    graded_outputs = eval_chain.evaluate(\r\n  File \"/Users/helen/code/xiaomi/auto-evaluator/venv/lib/python3.9/site-packages/langchain/evaluation/qa/eval_chain.py\", line 60, in evaluate\r\n    return self.apply(inputs)\r\n  File \"/Users/helen/code/xiaomi/auto-evaluator/venv/lib/python3.9/site-packages/langchain/chains/llm.py\", line 118, in apply\r\n    response = self.generate(input_list)\r\n  File \"/Users/helen/code/xiaomi/auto-evaluator/venv/lib/python3.9/site-packages/langchain/chains/llm.py\", line 62, in generate\r\n    return self.llm.generate_prompt(prompts, stop)\r\n  File \"/Users/helen/code/xiaomi/auto-evaluator/venv/lib/python3.9/site-packages/langchain/chat_models/base.py\", line 82, in generate_prompt\r\n    raise e\r\n  File \"/Users/helen/code/xiaomi/auto-evaluator/venv/lib/python3.9/site-packages/langchain/chat_models/base.py\", line 79, in generate_prompt\r\n    output = self.generate(prompt_messages, stop=stop)\r\n  File \"/Users/helen/code/xiaomi/auto-evaluator/venv/lib/python3.9/site-packages/langchain/chat_models/base.py\", line 54, in generate\r\n    results = [self._generate(m, stop=stop) for m in messages]\r\n  File \"/Users/helen/code/xiaomi/auto-evaluator/venv/lib/python3.9/site-packages/langchain/chat_models/base.py\", line 54, in <listcomp>\r\n    results = [self._generate(m, stop=stop) for m in messages]\r\n  File \"/Users/helen/code/xiaomi/auto-evaluator/venv/lib/python3.9/site-packages/langchain/chat_models/openai.py\", line 266, in _generate\r\n    response = self.completion_with_retry(messages=message_dicts, **params)\r\n  File \"/Users/helen/code/xiaomi/auto-evaluator/venv/lib/python3.9/site-packages/langchain/chat_models/openai.py\", line 228, in completion_with_retry\r\n    return _completion_with_retry(**kwargs)\r\n  File \"/Users/helen/code/xiaomi/auto-evaluator/venv/lib/python3.9/site-packages/tenacity/__init__.py\", line 289, in wrapped_f\r\n    return self(f, *args, **kw)\r\n  File \"/Users/helen/code/xiaomi/auto-evaluator/venv/lib/python3.9/site-packages/tenacity/__init__.py\", line 379, in __call__\r\n    do = self.iter(retry_state=retry_state)\r\n  File \"/Users/helen/code/xiaomi/auto-evaluator/venv/lib/python3.9/site-packages/tenacity/__init__.py\", line 314, in iter\r\n    return fut.result()\r\n  File \"/usr/local/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py\", line 439, in result\r\n    return self.__get_result()\r\n  File \"/usr/local/Cellar/python@3.9/3.9.12/Frameworks/Python.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py\", line 391, in __get_result\r\n    raise self._exception\r\n  File \"/Users/helen/code/xiaomi/auto-evaluator/venv/lib/python3.9/site-packages/tenacity/__init__.py\", line 382, in __call__\r\n    result = fn(*args, **kwargs)\r\n  File \"/Users/helen/code/xiaomi/auto-evaluator/venv/lib/python3.9/site-packages/langchain/chat_models/openai.py\", line 226, in _completion_with_retry\r\n    return self.client.create(**kwargs)\r\n  File \"/Users/helen/code/xiaomi/auto-evaluator/venv/lib/python3.9/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\r\n    return super().create(*args, **kwargs)\r\n  File \"/Users/helen/code/xiaomi/auto-evaluator/venv/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\r\n    response, _, api_key = requestor.request(\r\n  File \"/Users/helen/code/xiaomi/auto-evaluator/venv/lib/python3.9/site-packages/openai/api_requestor.py\", line 226, in request\r\n    resp, got_stream = self._interpret_response(result, stream)\r\n  File \"/Users/helen/code/xiaomi/auto-evaluator/venv/lib/python3.9/site-packages/openai/api_requestor.py\", line 619, in _interpret_response\r\n    self._interpret_response_line(\r\n  File \"/Users/helen/code/xiaomi/auto-evaluator/venv/lib/python3.9/site-packages/openai/api_requestor.py\", line 679, in _interpret_response_line\r\n    raise self.handle_error_response(\r\nopenai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4280 tokens. Please reduce the length of the messages.\r\n", "reactions": {"url": "https://api.github.com/repos/rlancemartin/auto-evaluator/issues/15/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/rlancemartin/auto-evaluator/issues/15/timeline", "performed_via_github_app": null, "state_reason": "completed", "score": 1.0}]}