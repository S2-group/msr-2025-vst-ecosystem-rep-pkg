{"total_count": 7, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/pytorch/QNNPACK/issues/73", "repository_url": "https://api.github.com/repos/pytorch/QNNPACK", "labels_url": "https://api.github.com/repos/pytorch/QNNPACK/issues/73/labels{/name}", "comments_url": "https://api.github.com/repos/pytorch/QNNPACK/issues/73/comments", "events_url": "https://api.github.com/repos/pytorch/QNNPACK/issues/73/events", "html_url": "https://github.com/pytorch/QNNPACK/issues/73", "id": 681635177, "node_id": "MDU6SXNzdWU2ODE2MzUxNzc=", "number": 73, "title": "Qnnpack accuracy very poor on unet model", "user": {"login": "Amitdedhia6", "id": 32384197, "node_id": "MDQ6VXNlcjMyMzg0MTk3", "avatar_url": "https://avatars.githubusercontent.com/u/32384197?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Amitdedhia6", "html_url": "https://github.com/Amitdedhia6", "followers_url": "https://api.github.com/users/Amitdedhia6/followers", "following_url": "https://api.github.com/users/Amitdedhia6/following{/other_user}", "gists_url": "https://api.github.com/users/Amitdedhia6/gists{/gist_id}", "starred_url": "https://api.github.com/users/Amitdedhia6/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Amitdedhia6/subscriptions", "organizations_url": "https://api.github.com/users/Amitdedhia6/orgs", "repos_url": "https://api.github.com/users/Amitdedhia6/repos", "events_url": "https://api.github.com/users/Amitdedhia6/events{/privacy}", "received_events_url": "https://api.github.com/users/Amitdedhia6/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [], "state": "open", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2020-08-19T08:05:03Z", "updated_at": "2020-08-21T09:03:10Z", "closed_at": null, "author_association": "NONE", "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "active_lock_reason": null, "body": "I am using Unet model for semantic segmentation. I pass a batch of images to the model. The model is expected to output 0 or 1 for each pixel of the image (depending upon whether pixel is part of person object or not). 0 is for background, and 1 is for foreground.\r\n\r\nI am trying to quantize the Unet model with Pytorch quantization apis for ARM architecture. I chose Qnnpack as quantization configuration. However the model accuracy is very poor for both Post training static quantization as well as QAT. The output is always a complete black image i.e. contains only background, no foreground for the person object. The model outputs bX2X224X224 i.e. batch_size X 2 channels (one for forground and one for background) X height X width.\r\n\r\nFollowing is the output values for the center pixels of images - with original model and with quantized model.\r\n![image](https://user-images.githubusercontent.com/32384197/90606892-15bfa980-e21e-11ea-8109-5207a759ff82.png)\r\n\r\n\r\nAs seen, the original model has varying output values. Hence when we apply Softmax on dim=1 (i,e, Channel dimension), we get some pixels as 0 and some as 1. This is as per expectation. However, the quantized model always outputs high positive for background and high negative for foreground channel. After applying softmax, all the pixels are background pixels, and the output is black images.\r\n\r\nI need some help to find why this is happening. Is it a bug in qnnpack quantization routine?\r\n\r\nThe model source code is available at [here](https://github.com/thuyngch/Human-Segmentation-PyTorch). I used pretrained version of  Unet with MobileNetV2 as backbone (check benchmark section in the readme from the source code link) - [see here](https://drive.google.com/file/d/17GZLCi_FHhWo4E4wPobbLAQdBZrlqVnF/view).\r\n\r\nI first tried with Fbgemm configuration, and it worked fine in terms of accuracy - no major loss. However, when tried with qnnpack, I face above issues. Following is my code for QAT.\r\n\r\n```\r\nuse_sigmoid = False\r\n\r\ndef poly_lr_scheduler(optimizer, init_lr, curr_iter, max_iter, power=0.9):\r\n    for g in optimizer.param_groups:\r\n        g['lr'] = init_lr * (1 - curr_iter/max_iter)**power\r\n\r\n\r\ndef dice_loss(logits, targets, smooth=1.0):\r\n    \"\"\"\r\n    logits: (torch.float32)  shape (N, C, H, W)\r\n    targets: (torch.float32) shape (N, H, W), value {0,1,...,C-1}\r\n    \"\"\"\r\n\r\n    if not use_sigmoid:\r\n        outputs = F.softmax(logits, dim=1)\r\n        targets = torch.unsqueeze(targets, dim=1)\r\n        # targets = torch.zeros_like(logits).scatter_(dim=1, index=targets.type(torch.int64), src=torch.tensor(1.0))\r\n        targets = torch.zeros_like(logits).scatter_(dim=1, index=targets.type(torch.int64),\r\n                                                    src=torch.ones(targets.type(torch.int64).shape))\r\n\r\n        inter = outputs * targets\r\n        dice = 1 - ((2 * inter.sum(dim=(2, 3)) + smooth) / (outputs.sum(dim=(2, 3)) + targets.sum(dim=(2, 3)) + smooth))\r\n        return dice.mean()\r\n    else:\r\n        outputs = logits[:,1,:,:]\r\n        outputs = torch.sigmoid(outputs)\r\n        inter = outputs * targets\r\n        dice = 1 - ((2*inter.sum(dim=(1,2)) + smooth) / (outputs.sum(dim=(1,2))+targets.sum(dim=(1,2)) + smooth))\r\n        return dice.mean()\r\n\r\ndef train_model_for_qat(model, data_loader, num_epochs, batch_size):\r\n    device = torch.device('cpu')\r\n    model_params = [p for p in model.parameters() if p.requires_grad]\r\n    SGD_params = {\r\n        \"lr\": 1e-2,\r\n        \"momentum\": 0.9,\r\n        \"weight_decay\": 1e-8\r\n    }\r\n\r\n    optimizer = torch.optim.SGD(model_params, lr=SGD_params[\"lr\"], momentum=SGD_params[\"momentum\"],\r\n                                nesterov=True, weight_decay=SGD_params[\"weight_decay\"])\r\n    init_lr = optimizer.param_groups[0]['lr']\r\n    scheduler = lr_scheduler.StepLR(optimizer, step_size=100, gamma=1)\r\n\r\n    max_iter = num_epochs * np.ceil(len(data_loader.dataset) / batch_size) + 5\r\n    curr_batch_num = 0\r\n\r\n    for epoch in range(num_epochs):\r\n        print(f\"Epoch {epoch} in progress\")\r\n        train_data_length = 0\r\n        total_train_loss = 0\r\n\r\n        model.train()\r\n        for batch in data_loader:\r\n            # get the next batch\r\n            data, target = batch\r\n            data, target = data.to(device), target.to(device)\r\n            train_data_length += len(batch[0])\r\n\r\n            optimizer.zero_grad()\r\n            outputs = model(data)\r\n            loss = dice_loss(outputs, target)\r\n            total_train_loss += loss.item()\r\n            loss.backward()\r\n            optimizer.step()\r\n            curr_batch_num += 1\r\n            poly_lr_scheduler(optimizer, init_lr, curr_batch_num, max_iter, power=0.9)\r\n\r\n        total_train_loss = total_train_loss / train_data_length\r\n        scheduler.step()\r\n\r\n        if epoch > 10:\r\n            # Freeze quantizer parameters\r\n            model.apply(Q.disable_observer)\r\n        if epoch > 20:\r\n            # Freeze batch norm mean and variance estimates\r\n            model.apply(nn_intrinsic_qat.freeze_bn_stats)\r\n\r\n        quantized_model = Q.convert(model.eval(), inplace=False)\r\n        accuracy = eval_model_for_quantization(quantized_model, device)\r\n        print(f\"...Accuacy at the end of epoch {epoch} : {accuracy}\")\r\n        if (accuracy > 99) and (epoch >= 10):\r\n            print(\"...GUESS we are done with training now...\")\r\n            break\r\n\r\n    return total_train_loss, model\r\n\r\n\r\n```\r\nAm I missing anything?\r\n\r\nOne issue that we did encounter is that the upsampling layers of Unet use nn.ConvTranspose2d which is not supported for quantization. Hence before this layer, we need to dequantize tensors, apply nn.ConvTranspose2d, and then requantize for subsequent layers. Can this be reason for lower accuracy?\r\n\r\n\r\n```\r\n#------------------------------------------------------------------------------\r\n#   Decoder block\r\n#------------------------------------------------------------------------------\r\nclass DecoderBlock(nn.Module):\r\n    def __init__(self, in_channels, out_channels, block_unit):\r\n        super(DecoderBlock, self).__init__()\r\n        self.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, padding=1, stride=2)\r\n        self.block_unit = block_unit\r\n        self.quant = QuantStub()\r\n        self.dequant = DeQuantStub()\r\n\r\n    def forward(self, input, shortcut):\r\n        # self.deconv = nn.ConvTranspose2d not supported for FBGEMM and QNNPACK quantization\r\n        input = self.dequant(input)\r\n        x = self.deconv(input)\r\n        x = self.quant(x)\r\n        x = torch.cat([x, shortcut], dim=1)\r\n        x = self.block_unit(x)\r\n        return x\r\n\r\n```\r\nThe following is the model after QAT training is completed for 30 epochs . . .\r\n\r\n```\r\nUNet(\r\n  (backbone): MobileNetV2(\r\n    (features): Sequential(\r\n      (0): Sequential(\r\n        (0): QuantizedConvReLU2d(3, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.012562132440507412, zero_point=0, padding=(1, 1))\r\n        (1): Identity()\r\n        (2): Identity()\r\n      )\r\n      (1): InvertedResidual(\r\n        (skip_add): QFunctional(\r\n          scale=1.0, zero_point=0\r\n          (activation_post_process): Identity()\r\n        )\r\n        (conv): Sequential(\r\n          (0): QuantizedConvReLU2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.046556487679481506, zero_point=0, padding=(1, 1), groups=32)\r\n          (1): Identity()\r\n          (2): Identity()\r\n          (3): QuantizedConv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), scale=0.043083205819129944, zero_point=96)\r\n          (4): Identity()\r\n        )\r\n      )\r\n      (2): InvertedResidual(\r\n        (skip_add): QFunctional(\r\n          scale=1.0, zero_point=0\r\n          (activation_post_process): Identity()\r\n        )\r\n        (conv): Sequential(\r\n          (0): QuantizedConvReLU2d(16, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.05470738932490349, zero_point=0)\r\n          (1): Identity()\r\n          (2): Identity()\r\n          (3): QuantizedConvReLU2d(96, 96, kernel_size=(3, 3), stride=(2, 2), scale=0.05578919127583504, zero_point=0, padding=(1, 1), groups=96)\r\n          (4): Identity()\r\n          (5): Identity()\r\n          (6): QuantizedConv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.08143805712461472, zero_point=131)\r\n          (7): Identity()\r\n        )\r\n      )\r\n      (3): InvertedResidual(\r\n        (skip_add): QFunctional(\r\n          scale=0.10905726253986359, zero_point=133\r\n          (activation_post_process): Identity()\r\n        )\r\n        (conv): Sequential(\r\n          (0): QuantizedConvReLU2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.021390624344348907, zero_point=0)\r\n          (1): Identity()\r\n          (2): Identity()\r\n          (3): QuantizedConvReLU2d(144, 144, kernel_size=(3, 3), stride=(1, 1), scale=0.03496978059411049, zero_point=0, padding=(1, 1), groups=144)\r\n          (4): Identity()\r\n          (5): Identity()\r\n          (6): QuantizedConv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.07988038659095764, zero_point=166)\r\n          (7): Identity()\r\n        )\r\n      )\r\n      (4): InvertedResidual(\r\n        (skip_add): QFunctional(\r\n          scale=1.0, zero_point=0\r\n          (activation_post_process): Identity()\r\n        )\r\n        (conv): Sequential(\r\n          (0): QuantizedConvReLU2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.016173962503671646, zero_point=0)\r\n          (1): Identity()\r\n          (2): Identity()\r\n          (3): QuantizedConvReLU2d(144, 144, kernel_size=(3, 3), stride=(2, 2), scale=0.05084317922592163, zero_point=0, padding=(1, 1), groups=144)\r\n          (4): Identity()\r\n          (5): Identity()\r\n          (6): QuantizedConv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.08057469874620438, zero_point=133)\r\n          (7): Identity()\r\n        )\r\n      )\r\n      (5): InvertedResidual(\r\n        (skip_add): QFunctional(\r\n          scale=0.07931466400623322, zero_point=141\r\n          (activation_post_process): Identity()\r\n        )\r\n        (conv): Sequential(\r\n          (0): QuantizedConvReLU2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.015451926738023758, zero_point=0)\r\n          (1): Identity()\r\n          (2): Identity()\r\n          (3): QuantizedConvReLU2d(192, 192, kernel_size=(3, 3), stride=(1, 1), scale=0.01901066116988659, zero_point=0, padding=(1, 1), groups=192)\r\n          (4): Identity()\r\n          (5): Identity()\r\n          (6): QuantizedConv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.03396213427186012, zero_point=137)\r\n          (7): Identity()\r\n        )\r\n      )\r\n      (6): InvertedResidual(\r\n        (skip_add): QFunctional(\r\n          scale=0.10119215399026871, zero_point=149\r\n          (activation_post_process): Identity()\r\n        )\r\n        (conv): Sequential(\r\n          (0): QuantizedConvReLU2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.009366143494844437, zero_point=0)\r\n          (1): Identity()\r\n          (2): Identity()\r\n          (3): QuantizedConvReLU2d(192, 192, kernel_size=(3, 3), stride=(1, 1), scale=0.03307875618338585, zero_point=0, padding=(1, 1), groups=192)\r\n          (4): Identity()\r\n          (5): Identity()\r\n          (6): QuantizedConv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.045690517872571945, zero_point=152)\r\n          (7): Identity()\r\n        )\r\n      )\r\n      (7): InvertedResidual(\r\n        (skip_add): QFunctional(\r\n          scale=1.0, zero_point=0\r\n          (activation_post_process): Identity()\r\n        )\r\n        (conv): Sequential(\r\n          (0): QuantizedConvReLU2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.013529903255403042, zero_point=0)\r\n          (1): Identity()\r\n          (2): Identity()\r\n          (3): QuantizedConvReLU2d(192, 192, kernel_size=(3, 3), stride=(2, 2), scale=0.030076880007982254, zero_point=0, padding=(1, 1), groups=192)\r\n          (4): Identity()\r\n          (5): Identity()\r\n          (6): QuantizedConv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.05553155764937401, zero_point=128)\r\n          (7): Identity()\r\n        )\r\n      )\r\n      (8): InvertedResidual(\r\n        (skip_add): QFunctional(\r\n          scale=0.057563915848731995, zero_point=132\r\n          (activation_post_process): Identity()\r\n        )\r\n        (conv): Sequential(\r\n          (0): QuantizedConvReLU2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.008955957368016243, zero_point=0)\r\n          (1): Identity()\r\n          (2): Identity()\r\n          (3): QuantizedConvReLU2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.01566135324537754, zero_point=0, padding=(1, 1), groups=384)\r\n          (4): Identity()\r\n          (5): Identity()\r\n          (6): QuantizedConv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.02868938073515892, zero_point=162)\r\n          (7): Identity()\r\n        )\r\n      )\r\n      (9): InvertedResidual(\r\n        (skip_add): QFunctional(\r\n          scale=0.05936211720108986, zero_point=140\r\n          (activation_post_process): Identity()\r\n        )\r\n        (conv): Sequential(\r\n          (0): QuantizedConvReLU2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.011350379325449467, zero_point=0)\r\n          (1): Identity()\r\n          (2): Identity()\r\n          (3): QuantizedConvReLU2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.013551343232393265, zero_point=0, padding=(1, 1), groups=384)\r\n          (4): Identity()\r\n          (5): Identity()\r\n          (6): QuantizedConv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.02829933725297451, zero_point=124)\r\n          (7): Identity()\r\n        )\r\n      )\r\n      (10): InvertedResidual(\r\n        (skip_add): QFunctional(\r\n          scale=0.056326691061258316, zero_point=121\r\n          (activation_post_process): Identity()\r\n        )\r\n        (conv): Sequential(\r\n          (0): QuantizedConvReLU2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.009888351894915104, zero_point=0)\r\n          (1): Identity()\r\n          (2): Identity()\r\n          (3): QuantizedConvReLU2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.00840410403907299, zero_point=0, padding=(1, 1), groups=384)\r\n          (4): Identity()\r\n          (5): Identity()\r\n          (6): QuantizedConv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.02762036770582199, zero_point=130)\r\n          (7): Identity()\r\n        )\r\n      )\r\n      (11): InvertedResidual(\r\n        (skip_add): QFunctional(\r\n          scale=1.0, zero_point=0\r\n          (activation_post_process): Identity()\r\n        )\r\n        (conv): Sequential(\r\n          (0): QuantizedConvReLU2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.010262548923492432, zero_point=0)\r\n          (1): Identity()\r\n          (2): Identity()\r\n          (3): QuantizedConvReLU2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.020638082176446915, zero_point=0, padding=(1, 1), groups=384)\r\n          (4): Identity()\r\n          (5): Identity()\r\n          (6): QuantizedConv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.03133825585246086, zero_point=114)\r\n          (7): Identity()\r\n        )\r\n      )\r\n      (12): InvertedResidual(\r\n        (skip_add): QFunctional(\r\n          scale=0.049823448061943054, zero_point=106\r\n          (activation_post_process): Identity()\r\n        )\r\n        (conv): Sequential(\r\n          (0): QuantizedConvReLU2d(96, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.007199177984148264, zero_point=0)\r\n          (1): Identity()\r\n          (2): Identity()\r\n          (3): QuantizedConvReLU2d(576, 576, kernel_size=(3, 3), stride=(1, 1), scale=0.017748937010765076, zero_point=0, padding=(1, 1), groups=576)\r\n          (4): Identity()\r\n          (5): Identity()\r\n          (6): QuantizedConv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.045204587280750275, zero_point=94)\r\n          (7): Identity()\r\n        )\r\n      )\r\n      (13): InvertedResidual(\r\n        (skip_add): QFunctional(\r\n          scale=0.06418105959892273, zero_point=125\r\n          (activation_post_process): Identity()\r\n        )\r\n        (conv): Sequential(\r\n          (0): QuantizedConvReLU2d(96, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.008789398707449436, zero_point=0)\r\n          (1): Identity()\r\n          (2): Identity()\r\n          (3): QuantizedConvReLU2d(576, 576, kernel_size=(3, 3), stride=(1, 1), scale=0.019841214641928673, zero_point=0, padding=(1, 1), groups=576)\r\n          (4): Identity()\r\n          (5): Identity()\r\n          (6): QuantizedConv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.06256742030382156, zero_point=129)\r\n          (7): Identity()\r\n        )\r\n      )\r\n      (14): InvertedResidual(\r\n        (skip_add): QFunctional(\r\n          scale=1.0, zero_point=0\r\n          (activation_post_process): Identity()\r\n        )\r\n        (conv): Sequential(\r\n          (0): QuantizedConvReLU2d(96, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.011278725229203701, zero_point=0)\r\n          (1): Identity()\r\n          (2): Identity()\r\n          (3): QuantizedConvReLU2d(576, 576, kernel_size=(3, 3), stride=(2, 2), scale=0.028320688754320145, zero_point=0, padding=(1, 1), groups=576)\r\n          (4): Identity()\r\n          (5): Identity()\r\n          (6): QuantizedConv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), scale=0.06365438550710678, zero_point=132)\r\n          (7): Identity()\r\n        )\r\n      )\r\n      (15): InvertedResidual(\r\n        (skip_add): QFunctional(\r\n          scale=0.08667448908090591, zero_point=127\r\n          (activation_post_process): Identity()\r\n        )\r\n        (conv): Sequential(\r\n          (0): QuantizedConvReLU2d(160, 960, kernel_size=(1, 1), stride=(1, 1), scale=0.011708680540323257, zero_point=0)\r\n          (1): Identity()\r\n          (2): Identity()\r\n          (3): QuantizedConvReLU2d(960, 960, kernel_size=(3, 3), stride=(1, 1), scale=0.026726122945547104, zero_point=0, padding=(1, 1), groups=960)\r\n          (4): Identity()\r\n          (5): Identity()\r\n          (6): QuantizedConv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), scale=0.04459201171994209, zero_point=116)\r\n          (7): Identity()\r\n        )\r\n      )\r\n      (16): InvertedResidual(\r\n        (skip_add): QFunctional(\r\n          scale=0.1879616528749466, zero_point=126\r\n          (activation_post_process): Identity()\r\n        )\r\n        (conv): Sequential(\r\n          (0): QuantizedConvReLU2d(160, 960, kernel_size=(1, 1), stride=(1, 1), scale=0.015011523850262165, zero_point=0)\r\n          (1): Identity()\r\n          (2): Identity()\r\n          (3): QuantizedConvReLU2d(960, 960, kernel_size=(3, 3), stride=(1, 1), scale=0.025075148791074753, zero_point=0, padding=(1, 1), groups=960)\r\n          (4): Identity()\r\n          (5): Identity()\r\n          (6): QuantizedConv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), scale=0.1145012229681015, zero_point=119)\r\n          (7): Identity()\r\n        )\r\n      )\r\n      (17): InvertedResidual(\r\n        (skip_add): QFunctional(\r\n          scale=1.0, zero_point=0\r\n          (activation_post_process): Identity()\r\n        )\r\n        (conv): Sequential(\r\n          (0): QuantizedConvReLU2d(160, 960, kernel_size=(1, 1), stride=(1, 1), scale=0.006071502808481455, zero_point=0)\r\n          (1): Identity()\r\n          (2): Identity()\r\n          (3): QuantizedConvReLU2d(960, 960, kernel_size=(3, 3), stride=(1, 1), scale=0.01608050987124443, zero_point=0, padding=(1, 1), groups=960)\r\n          (4): Identity()\r\n          (5): Identity()\r\n          (6): QuantizedConv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), scale=0.02348274178802967, zero_point=127)\r\n          (7): Identity()\r\n        )\r\n      )\r\n      (18): Sequential(\r\n        (0): QuantizedConvReLU2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), scale=0.08627913892269135, zero_point=0)\r\n        (1): Identity()\r\n        (2): Identity()\r\n      )\r\n    )\r\n  )\r\n  (decoder1): DecoderBlock(\r\n    (deconv): ConvTranspose2d(1280, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\r\n    (block_unit): InvertedResidual(\r\n      (skip_add): QFunctional(\r\n        scale=1.0, zero_point=0\r\n        (activation_post_process): Identity()\r\n      )\r\n      (conv): Sequential(\r\n        (0): QuantizedConvReLU2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), scale=0.038529299199581146, zero_point=0)\r\n        (1): Identity()\r\n        (2): Identity()\r\n        (3): QuantizedConvReLU2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), scale=0.06666766107082367, zero_point=0, padding=(1, 1), groups=1152)\r\n        (4): Identity()\r\n        (5): Identity()\r\n        (6): QuantizedConv2d(1152, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.0864308550953865, zero_point=117)\r\n        (7): Identity()\r\n      )\r\n    )\r\n    (quant): Quantize(scale=tensor([0.0979]), zero_point=tensor([128]), dtype=torch.quint8)\r\n    (dequant): DeQuantize()\r\n  )\r\n  (decoder2): DecoderBlock(\r\n    (deconv): ConvTranspose2d(96, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\r\n    (block_unit): InvertedResidual(\r\n      (skip_add): QFunctional(\r\n        scale=1.0, zero_point=0\r\n        (activation_post_process): Identity()\r\n      )\r\n      (conv): Sequential(\r\n        (0): QuantizedConvReLU2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.06379921734333038, zero_point=0)\r\n        (1): Identity()\r\n        (2): Identity()\r\n        (3): QuantizedConvReLU2d(384, 384, kernel_size=(3, 3), stride=(1, 1), scale=0.28728926181793213, zero_point=0, padding=(1, 1), groups=384)\r\n        (4): Identity()\r\n        (5): Identity()\r\n        (6): QuantizedConv2d(384, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.2210002988576889, zero_point=126)\r\n        (7): Identity()\r\n      )\r\n    )\r\n    (quant): Quantize(scale=tensor([0.0561]), zero_point=tensor([120]), dtype=torch.quint8)\r\n    (dequant): DeQuantize()\r\n  )\r\n  (decoder3): DecoderBlock(\r\n    (deconv): ConvTranspose2d(32, 24, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\r\n    (block_unit): InvertedResidual(\r\n      (skip_add): QFunctional(\r\n        scale=1.0, zero_point=0\r\n        (activation_post_process): Identity()\r\n      )\r\n      (conv): Sequential(\r\n        (0): QuantizedConvReLU2d(48, 288, kernel_size=(1, 1), stride=(1, 1), scale=0.11421681195497513, zero_point=0)\r\n        (1): Identity()\r\n        (2): Identity()\r\n        (3): QuantizedConvReLU2d(288, 288, kernel_size=(3, 3), stride=(1, 1), scale=0.20177718997001648, zero_point=0, padding=(1, 1), groups=288)\r\n        (4): Identity()\r\n        (5): Identity()\r\n        (6): QuantizedConv2d(288, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.21056368947029114, zero_point=114)\r\n        (7): Identity()\r\n      )\r\n    )\r\n    (quant): Quantize(scale=tensor([0.1462]), zero_point=tensor([123]), dtype=torch.quint8)\r\n    (dequant): DeQuantize()\r\n  )\r\n  (decoder4): DecoderBlock(\r\n    (deconv): ConvTranspose2d(24, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\r\n    (block_unit): InvertedResidual(\r\n      (skip_add): QFunctional(\r\n        scale=1.0, zero_point=0\r\n        (activation_post_process): Identity()\r\n      )\r\n      (conv): Sequential(\r\n        (0): QuantizedConvReLU2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.1248798817396164, zero_point=0)\r\n        (1): Identity()\r\n        (2): Identity()\r\n        (3): QuantizedConvReLU2d(192, 192, kernel_size=(3, 3), stride=(1, 1), scale=0.1945924311876297, zero_point=0, padding=(1, 1), groups=192)\r\n        (4): Identity()\r\n        (5): Identity()\r\n        (6): QuantizedConv2d(192, 16, kernel_size=(1, 1), stride=(1, 1), scale=0.3521292805671692, zero_point=135)\r\n        (7): Identity()\r\n      )\r\n    )\r\n    (quant): Quantize(scale=tensor([0.1450]), zero_point=tensor([140]), dtype=torch.quint8)\r\n    (dequant): DeQuantize()\r\n  )\r\n  (conv_last): Sequential(\r\n    (0): QuantizedConv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), scale=1.0138226747512817, zero_point=131, padding=(1, 1))\r\n    (1): QuantizedConv2d(3, 2, kernel_size=(3, 3), stride=(1, 1), scale=2.995656728744507, zero_point=128, padding=(1, 1))\r\n  )\r\n  (quant): Quantize(scale=tensor([0.0186]), zero_point=tensor([114]), dtype=torch.quint8)\r\n  (dequant): DeQuantize()\r\n)\r\n```\r\nAny help is highly appreciated - thanks.", "reactions": {"url": "https://api.github.com/repos/pytorch/QNNPACK/issues/73/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/pytorch/QNNPACK/issues/73/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}]}