{"total_count": 45, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/Atome-FE/llama-node/issues/124", "repository_url": "https://api.github.com/repos/Atome-FE/llama-node", "labels_url": "https://api.github.com/repos/Atome-FE/llama-node/issues/124/labels{/name}", "comments_url": "https://api.github.com/repos/Atome-FE/llama-node/issues/124/comments", "events_url": "https://api.github.com/repos/Atome-FE/llama-node/issues/124/events", "html_url": "https://github.com/Atome-FE/llama-node/issues/124", "id": 2203881978, "node_id": "I_kwDOJL272s6DXJH6", "number": 124, "title": "What files are compatible?", "user": {"login": "bedcoding", "id": 33927558, "node_id": "MDQ6VXNlcjMzOTI3NTU4", "avatar_url": "https://avatars.githubusercontent.com/u/33927558?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bedcoding", "html_url": "https://github.com/bedcoding", "followers_url": "https://api.github.com/users/bedcoding/followers", "following_url": "https://api.github.com/users/bedcoding/following{/other_user}", "gists_url": "https://api.github.com/users/bedcoding/gists{/gist_id}", "starred_url": "https://api.github.com/users/bedcoding/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bedcoding/subscriptions", "organizations_url": "https://api.github.com/users/bedcoding/orgs", "repos_url": "https://api.github.com/users/bedcoding/repos", "events_url": "https://api.github.com/users/bedcoding/events{/privacy}", "received_events_url": "https://api.github.com/users/bedcoding/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [], "state": "open", "locked": true, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2024-03-23T13:32:53Z", "updated_at": "2024-03-23T13:32:53Z", "closed_at": null, "author_association": "NONE", "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "active_lock_reason": null, "body": "I have read the document below.\r\nhttps://llama-node.vercel.app/docs/start\r\n\r\nSo, I looked for a model to use in the source code at the site below.\r\nhttps://huggingface.co/models?search=ggml\r\n\r\nThen an error like this appears. What files should I get?\r\n```\r\nllama.cpp: loading model from model/llama-2-7b-chat.ggmlv3.q2_K.bin\r\nerror loading model: unrecognized tensor type 10\r\n\r\nllama_init_from_file: failed to load model\r\nnode:internal/process/promises:289\r\n            triggerUncaughtException(err, true /* fromPromise */);\r\n            ^\r\n\r\n[Error: Failed to initialize LLama context from file: model/llama-2-7b-chat.ggmlv3.q2_K.bin] {\r\n  code: 'GenericFailure'\r\n}\r\n```\r\n\r\n\r\ncode: \r\n```\r\nimport { LLM } from \"llama-node\";\r\nimport { LLamaCpp } from \"llama-node/dist/llm/llama-cpp.js\";\r\n\r\nconst model = \"model/llama-2-7b-chat.ggmlv3.q2_K.bin\";\r\nconst llama = new LLM(LLamaCpp);\r\nconst config = {\r\n    modelPath: model,\r\n    enableLogging: true,\r\n    nCtx: 1024,\r\n    seed: 0,\r\n    f16Kv: false,\r\n    logitsAll: false,\r\n    vocabOnly: false,\r\n    useMlock: false,\r\n    embedding: false,\r\n    useMmap: true,\r\n    nGpuLayers: 0\r\n};\r\n\r\nconst template = `How are you?`;\r\nconst prompt = `A chat between a user and an assistant.\r\nUSER: ${template}\r\nASSISTANT:`;\r\n\r\nconst run = async () => {\r\n  await llama.load(config);\r\n\r\n  await llama.createCompletion({\r\n      nThreads: 4,\r\n      nTokPredict: 2048,\r\n      topK: 40,\r\n      topP: 0.1,\r\n      temp: 0.2,\r\n      repeatPenalty: 1,\r\n      prompt,\r\n  }, (response) => {\r\n      process.stdout.write(response.token);\r\n  });\r\n}\r\n\r\nrun();\r\n```", "reactions": {"url": "https://api.github.com/repos/Atome-FE/llama-node/issues/124/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/Atome-FE/llama-node/issues/124/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}]}