{"total_count": 8, "incomplete_results": false, "items": [{"url": "https://api.github.com/repos/PotatoSpudowski/fastLLaMa/issues/86", "repository_url": "https://api.github.com/repos/PotatoSpudowski/fastLLaMa", "labels_url": "https://api.github.com/repos/PotatoSpudowski/fastLLaMa/issues/86/labels{/name}", "comments_url": "https://api.github.com/repos/PotatoSpudowski/fastLLaMa/issues/86/comments", "events_url": "https://api.github.com/repos/PotatoSpudowski/fastLLaMa/issues/86/events", "html_url": "https://github.com/PotatoSpudowski/fastLLaMa/issues/86", "id": 2270287249, "node_id": "I_kwDOJMfroc6HUdWR", "number": 86, "title": "GGUF and/or LLama-3 support?", "user": {"login": "BHX2", "id": 1174898, "node_id": "MDQ6VXNlcjExNzQ4OTg=", "avatar_url": "https://avatars.githubusercontent.com/u/1174898?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BHX2", "html_url": "https://github.com/BHX2", "followers_url": "https://api.github.com/users/BHX2/followers", "following_url": "https://api.github.com/users/BHX2/following{/other_user}", "gists_url": "https://api.github.com/users/BHX2/gists{/gist_id}", "starred_url": "https://api.github.com/users/BHX2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BHX2/subscriptions", "organizations_url": "https://api.github.com/users/BHX2/orgs", "repos_url": "https://api.github.com/users/BHX2/repos", "events_url": "https://api.github.com/users/BHX2/events{/privacy}", "received_events_url": "https://api.github.com/users/BHX2/received_events", "type": "User", "user_view_type": "public", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2024-04-30T00:56:34Z", "updated_at": "2024-04-30T00:57:26Z", "closed_at": null, "author_association": "NONE", "sub_issues_summary": {"total": 0, "completed": 0, "percent_completed": 0}, "active_lock_reason": null, "body": "Is this project updated enough to use gguf files or the LLama-3 architecture? I see that the documentation examples use ggml via .bin files which I'm assuming was the previous file format. I'm specifically interested in the loading / unloading feature for LoRa feature that doesn't seem supported in llama.cpp by itself", "reactions": {"url": "https://api.github.com/repos/PotatoSpudowski/fastLLaMa/issues/86/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/PotatoSpudowski/fastLLaMa/issues/86/timeline", "performed_via_github_app": null, "state_reason": null, "score": 1.0}]}